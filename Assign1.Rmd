---
title: "Assignment"
output: html_document
---

### Part (I): Load Data

```{r}
library(RWeka)
library(caret)
library(adabag)
library(randomForest)
library(nnet)
library(ipred)
library(klaR)
library(dprep)
df <- read.csv("sonar.all-data")

```

### Part (II): Evaluate Decision Tree model
#### A model is created using J48 method from the RWeka library which implements the C4.5 algorithm for decision trees. Since in this part we want to evaluate the model once without cross validation and once with 10-folds cross validation on the same training dataset, the evaluate method could be used with specifying the numFolds and outputting a summary of the evaluation metrics as well as the confusion matrix.

```{r}
sonar_j48 <- J48(R ~ ., data = df)


evaluate_Weka_classifier(sonar_j48, numFolds = 0, complexity = FALSE, 
                                     class = TRUE)

evaluate_Weka_classifier(sonar_j48, numFolds = 10, complexity = FALSE, 
                                     class = TRUE)

```

### Part (III):
#### Here we want to compare the performance of the different classifiers. after each classifier we print the Sensitivity which is equal to the recall of the first class, Positive Predictive Value is the Precision

```{r warning=FALSE, message=FALSE , results="hide" }
mymat = matrix(nrow=6, ncol=4)

for(i in 1:dim(mymat)[1])  
{
	if(i==1){
	m <- "rf"
	}else if(i==2){
	m <- "nb"
	}else if(i==3){
	m <- "nnet"
	}else if(i==4){
	m <- "svmLinear"
	}else if(i==5){
	  m <- "treebag"
	}else{
	  m <- "blackboost"
	}
	split=0.80
bound <- floor(nrow(df)*0.8)         
df <- df[sample(nrow(df)), ]         
data_train <- df[1:bound, ]              
data_test <- df[(bound+1):nrow(df), ] 
train_control <- trainControl(method="cv", number=10 )
model <- train(R~., data=data_train, trControl=train_control, method=m)
p <- predict(model,newdata = data_test[,1:60])
t <- table(p , data_test[,61])
cm <- confusionMatrix(t)
acc <- cm$overall['Accuracy']
precision <- cm$byClass['Pos Pred Value'] 
recall <- cm$byClass['Sensitivity']
f_measure <- 2 * ((precision * recall) / (precision + recall))
  for(j in 1:dim(mymat)[2])
  {
    mymat[i,1] = acc
    mymat[i,2] = precision
    mymat[i,3] = recall
    mymat[i,4] = f_measure
  }
}
colnames(mymat) <- c('accuracy','precision','recall','f_measure')
rownames(mymat) <- c('random.forest', 'naive.bayes','neural.network','svm','treebag','blackboost')
mytable <- as.table(mymat)

```

```{r}
mytable
```

### Part (IV) : Expermenting with different datasets.
#### prepare the new datasets

```{r}
hepatitis <- read.csv("hepatitis.data")
diabetes <- read.csv("pima-indians-diabetes.data")
SPECT.test <- read.csv("SPECT.test")
SPECT.train <- read.csv("SPECT.train")
colnames(SPECT.train) <- colnames(SPECT.test) 
```

#### Train the models on each dataset training part with each algorithm using cross validation, and test on testing part

```{r warning=FALSE, message=FALSE , results="hide" }
mymat = matrix(nrow=3, ncol=6)
mymatp = matrix(nrow=3, ncol=6)
mymatr = matrix(nrow=3, ncol=6)
mymatf = matrix(nrow=3, ncol=6)
for(i in 1:dim(mymat)[1])  
{
if(i==1){
df <- hepatitis
split=0.80
bound <- floor(nrow(df)*0.8)         
df <- df[sample(nrow(df)), ]          
data_train <- df[1:bound, ]             
data_test <- df[(bound+1):nrow(df), ] 
}else if(i==2){
df <- diabetes
split=0.80
bound <- floor(nrow(df)*0.8)        
df <- df[sample(nrow(df)), ]           
data_train <- df[1:bound, ]              
data_test <- df[(bound+1):nrow(df), ] 
}else{
data_train <- SPECT.train
data_test <- SPECT.test
}
for(j in 1:dim(mymat)[2]) 
  {
  	if(j==1){
	m <- "rf"
	}else if(j==2){
	m <- "nb"
	}else if(j==3){
	m <- "nnet"
	}else if(j==4){
	m <- "svmLinear"
	}else if(j==5){
	  m <- "treebag"
	}else{
	  m <- "blackboost"
	}
   train_control <- trainControl(method="cv", number=10 )
	if(i==1){  # hep
	 
 X2 <- data_train$X2
   data_train$X2 <- factor(X2)
	 	model <- train(X2~., data=data_train, trControl=train_control, method=m)
	p <- predict(model,newdata = data_test[,-1])
	t <- table(p , data_test[,1])
	}else if(i==3){ #Spect
	   X1 <- data_train$X1
   data_train$X1 <- factor(X1)
	model <- train(X1~., data=data_train, trControl=train_control, method=m )
	p <- predict(model,newdata = data_test[,-1])
	t <- table(p , data_test[,1])
	}
	 else{ #diabetes
  	X1 <- data_train$X1
    data_train$X1 <- factor(X1)
  	model <- train(X1~., data=data_train, trControl=train_control, method=m)
  	p <- predict(model,newdata = data_test[,-9])
  	t <- table(p , data_test[,9])
	}
	cm <- confusionMatrix(t)
	acc <- cm$overall['Accuracy']
	if(i==1){ #Hepatities , class 1 corresponds to die and its the positive class
	precision <- cm$byClass['Pos Pred Value'] 
  recall <- cm$byClass['Sensitivity']
}else{ # for SPECT and Diabetes we want to consider the class 1 , where the patient tested positive for the disease
  precision <- cm$byClass['Neg Pred Value'] 
  recall <- cm$byClass['Specificity']
}
  f_measure <- 2 * ((precision * recall) / (precision + recall))
	mymat[i,j] = acc
	mymatp[i,j] = precision
	mymatr[i,j] = recall
	mymatf[i,j] = f_measure
  }
}
rownames(mymat) <- c('hepatits','diabetes','SPECT')
colnames(mymat) <- c('random.forest', 'naive.bayes','neural.network','svm','treebag','blackboost')
rownames(mymatp) <- c('hepatits','diabetes','SPECT')
colnames(mymatp) <- c('random.forest', 'naive.bayes','neural.network','svm','treebag','blackboost')
rownames(mymatr) <- c('hepatits','diabetes','SPECT')
colnames(mymatr) <- c('random.forest', 'naive.bayes','neural.network','svm','treebag','blackboost')
rownames(mymatf) <- c('hepatits','diabetes','SPECT')
colnames(mymatf) <- c('random.forest', 'naive.bayes','neural.network','svm','treebag','blackboost')
acctable <- as.table(mymat)
precision.table <- as.table(mymatp)
recall.table <- as.table(mymatr)
f.table <- as.table(mymatf)
```

#### Accuracy table, precision table, recall table an f-measurment table

```{r}
acctable
precision.table
recall.table
f.table
```

### Conclusions and Remarks
- for the Hepatits dataset class 1 is considered. The classification of "Die" . (the positive class in confusion matrix)
- for the SPECT and Diabetes datasets class 1 are considered. The classification of patients that are classified positive for the disease. (the negative classes in confusion matrix)
- the approach used is to have a part of the dataset for training and other for testing, in the case of SPECT there were two seperate datasets while for the other datasets splitting was performed. After that training is performed in addition to 10-fold cross validation, then testing was performed.
- there were some missing values in the Hepatits dataset.
- from the percpective of the performance of the model on the test dataset, the accuracy, precision, recall and F-measurment were considered.
- the repetition of the experiment sometimes lead to slightly different results which could be accounted to the tuning and using cross validation with training which picks random subsets as folds. also the nature of some algorithms like random forest where a new model is learned.
- there is no overall winner, but in general the "SVM" maintains a good ranking in most of the performance measurments.
